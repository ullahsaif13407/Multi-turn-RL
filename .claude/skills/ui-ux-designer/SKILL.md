---
name: ui-ux-designer
description: Design user interfaces for RL training dashboards, experiment management, and visualization tools. Use when planning UI layouts, designing user flows, or creating wireframes for RL applications.
---

# UI/UX Designer - RL Dashboard & Interface Design

Design intuitive, informative interfaces for RL training, monitoring, and experimentation.

## When to Use This Skill

- Designing training dashboard layouts
- Creating user flows for experiment management
- Planning data visualizations for RL metrics
- Designing environment configuration interfaces
- Creating wireframes for hyperparameter tuning
- Improving existing UI based on user feedback

## Core Principles for RL UI Design

### 1. Information Hierarchy

**Primary Information** (Always visible)
- Training status (running/paused/completed)
- Current timestep / Total timesteps
- Live training reward (episode and smoothed)
- Agent performance (success rate, episode length)

**Secondary Information** (Accessible, not always visible)
- Hyperparameter values
- Network architecture details
- Environment configuration
- Checkpoint history

**Tertiary Information** (On-demand)
- Detailed logs
- Episode replays
- Gradient statistics
- System resource usage

### 2. Real-Time Feedback

Users need immediate feedback on:
- Training progress (progress bar + ETA)
- Agent performance trends (live charts)
- System health (GPU/CPU usage, memory)
- Errors or warnings (prominent alerts)

### 3. Contextual Actions

Actions should be:
- **Contextual**: "Pause Training" button only when training is active
- **Reversible**: "Resume Training" after pause
- **Confirmed**: "Stop Training" requires confirmation (data loss)
- **Grouped**: Related actions together (Start/Pause/Stop)

### 4. Progressive Disclosure

Don't overwhelm users:
- Start with essentials (training status, reward curve)
- Expand sections on demand (detailed metrics, logs)
- Use tabs/accordion for complex information
- Provide "Advanced Settings" for expert users

## User Personas for RL Systems

### 1. Researcher/Student
**Goals:**
- Experiment with different algorithms
- Understand agent behavior
- Compare hyperparameter effects

**Needs:**
- Easy algorithm selection
- Clear metric visualizations
- Experiment comparison tools
- Episode replay capability

### 2. ML Engineer
**Goals:**
- Train production agents
- Monitor training stability
- Optimize hyperparameters
- Deploy trained models

**Needs:**
- Robust training pipeline
- Checkpoint management
- Performance profiling
- Model export functionality

### 3. Domain Expert (Non-ML)
**Goals:**
- Train agents for specific tasks
- Evaluate agent behavior
- Adjust reward functions

**Needs:**
- Simplified configuration
- Intuitive environment setup
- Behavior visualization
- Pre-built templates

## Key UI Components for RL Systems

### 1. Training Dashboard

**Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Status Bar                                         â”‚
â”‚ CartPole-v1 | PPO | Running | 45,000 / 100,000 steps       â”‚
â”‚ [Pause] [Stop] [Checkpoint]                      45% â– â– â– â– â–«â–«â–«â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Episode Reward                   â”‚ Episode Length           â”‚
â”‚                                  â”‚                          â”‚
â”‚  500 â”¤                      â€¢    â”‚  200 â”¤            â€¢      â”‚
â”‚      â”‚                   â€¢       â”‚      â”‚         â€¢         â”‚
â”‚  250 â”¤              â€¢            â”‚  100 â”¤    â€¢              â”‚
â”‚      â”‚         â€¢                 â”‚      â”‚ â€¢                 â”‚
â”‚    0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚    0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚      0     25k    50k    75k     â”‚      0    25k   50k  75kâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Policy Loss                      â”‚ Value Loss               â”‚
â”‚ (Recent: 0.032, Smoothed: 0.041) â”‚ (Recent: 0.089)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Quick Stats                                                 â”‚
â”‚ Success Rate: 87% | Avg Reward: 421.3 | FPS: 2,341         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features:**
- Real-time updating charts (1-5 second refresh)
- Smoothed lines with confidence intervals
- Tooltips on hover showing exact values
- Zoom/pan functionality for detailed inspection
- Export chart as image

### 2. Experiment Configuration

**Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create New Experiment                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ Experiment Name: [my_experiment_001____________]            â”‚
â”‚                                                             â”‚
â”‚ Environment:  [CartPole-v1 â–¼]                               â”‚
â”‚               Classic control task: Balance pole on cart    â”‚
â”‚                                                             â”‚
â”‚ Algorithm:    [PPO â–¼]                                       â”‚
â”‚               On-policy, works well for continuous control  â”‚
â”‚                                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚ â”‚ Hyperparameters                          [Use Defaults]  â”‚
â”‚ â”‚                                                         â”‚ â”‚
â”‚ â”‚ Learning Rate:    [0.0003___]                           â”‚ â”‚
â”‚ â”‚ Rollout Steps:    [2048_____]                           â”‚ â”‚
â”‚ â”‚ Batch Size:       [64_______]                           â”‚ â”‚
â”‚ â”‚ Discount (Î³):     [0.99_____]                           â”‚ â”‚
â”‚ â”‚                                                         â”‚ â”‚
â”‚ â”‚ [â–¸ Advanced Settings]                                   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â”‚ Training Budget:                                            â”‚
â”‚ â—‹ Timesteps:  [100000__]                                    â”‚
â”‚ â—‹ Episodes:   [500_____]                                    â”‚
â”‚ â—‹ Wall Time:  [2_______] hours                              â”‚
â”‚                                                             â”‚
â”‚                          [Cancel]  [Start Training]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Design Principles:**
- Sensible defaults pre-filled
- Contextual help text for each field
- Validation on input (immediate feedback)
- Templates for common scenarios
- "Advanced Settings" collapsed by default

### 3. Experiment Comparison

**Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Compare Experiments                        [+ Add Experiment]â”‚
â”‚                                                             â”‚
â”‚ Selected: [âœ“] exp_lr_0.001  [âœ“] exp_lr_0.0003  [ ] exp_lr_0â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Episode Reward Comparison                                   â”‚
â”‚                                                             â”‚
â”‚  500 â”¤                                                      â”‚
â”‚      â”‚             â•±â•²    â”€â”€ exp_lr_0.001                    â”‚
â”‚  250 â”¤        â•±â”€â”€â”€â•¯  â•²                                      â”‚
â”‚      â”‚   â•±â”€â”€â”€â•¯        â•²â•²  Â·Â·Â· exp_lr_0.0003                â”‚
â”‚    0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚      0        25k         50k          75k                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Experiment           â”‚ Final Reward         â”‚ Timesteps    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ exp_lr_0.001         â”‚ 487.2 Â± 12.3         â”‚ 52,000       â”‚
â”‚ exp_lr_0.0003        â”‚ 421.5 Â± 18.7         â”‚ 68,000       â”‚
â”‚ exp_lr_0.0001        â”‚ 312.4 Â± 45.2         â”‚ 95,000       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features:**
- Select multiple experiments to compare
- Overlay plots with distinct colors/styles
- Statistical summary table
- Highlight best performing configuration

### 4. Agent Behavior Visualization

**Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Episode Replay: Episode #1234                  [â† â†’]        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Timeline               â”‚
â”‚   â”‚                             â”‚   0:00 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 0:45 â”‚
â”‚   â”‚     [Environment Render]    â”‚                           â”‚
â”‚   â”‚                             â”‚   Frame: 142 / 1024       â”‚
â”‚   â”‚         ðŸš— ðŸ              â”‚                           â”‚
â”‚   â”‚                             â”‚   [â® âª â¯ â© â­]         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                             â”‚
â”‚   Observation:                    Action:                   â”‚
â”‚   Position:  [0.42, -0.18]        Steer: 0.32              â”‚
â”‚   Velocity:  [1.23, 0.05]         Accel: 0.87              â”‚
â”‚   Angle:     0.12 rad                                       â”‚
â”‚                                                             â”‚
â”‚   Q-Values:              Policy Distribution:               â”‚
â”‚   Action 0: â–ªâ–ªâ–ªâ–ªâ–ªâ–ª 0.67  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘] 78%          â”‚
â”‚   Action 1: â–ªâ–ªâ–ªâ–ª 0.45     [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 15%          â”‚
â”‚   Action 2: â–ªâ–ª 0.23       [â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 7%           â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features:**
- Video playback controls
- Step-by-step inspection
- Observation/action display
- Policy/value visualization
- Export episode as video

### 5. Hyperparameter Tuning Interface

**Layout:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hyperparameter Tuning                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚ Base Configuration: [ppo_default â–¼]                         â”‚
â”‚                                                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚ â”‚ Parameters to Tune                                      â”‚ â”‚
â”‚ â”‚                                                         â”‚ â”‚
â”‚ â”‚ [âœ“] Learning Rate                                       â”‚ â”‚
â”‚ â”‚     Range: [1e-5____] to [1e-2____]  Scale: [Log â–¼]    â”‚ â”‚
â”‚ â”‚                                                         â”‚ â”‚
â”‚ â”‚ [âœ“] Batch Size                                          â”‚ â”‚
â”‚ â”‚     Options: [32, 64, 128, 256]                         â”‚ â”‚
â”‚ â”‚                                                         â”‚ â”‚
â”‚ â”‚ [ ] Discount Factor                                     â”‚ â”‚
â”‚ â”‚     Range: [0.95___] to [0.999__]  Scale: [Linear â–¼]   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                             â”‚
â”‚ Search Strategy:  â—‹ Grid Search  â— Random Search           â”‚
â”‚                   â—‹ Bayesian Optimization                   â”‚
â”‚                                                             â”‚
â”‚ Number of Trials: [20______]                                â”‚
â”‚ Parallel Runs:    [4_______]                                â”‚
â”‚                                                             â”‚
â”‚                            [Cancel]  [Start Tuning]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## User Flows

### Flow 1: New User - First Training Run

```
1. Landing Page
   â†“
2. "Quick Start" Tutorial
   - Choose environment (pre-populated: CartPole)
   - Choose algorithm (pre-populated: PPO)
   - Click "Start Training"
   â†“
3. Training Dashboard
   - Watch live reward curve
   - See tooltips explaining metrics
   â†“
4. Training Complete
   - Success message
   - "View Results" button
   â†“
5. Results Page
   - Final reward
   - Success rate
   - "Watch Episode Replay" button
```

### Flow 2: Researcher - Experiment Comparison

```
1. Experiments List
   - See all past experiments
   - Filter by environment/algorithm
   â†“
2. Select 2-3 experiments
   - Checkboxes for multi-select
   â†“
3. Click "Compare"
   â†“
4. Comparison Dashboard
   - Overlaid reward curves
   - Statistical comparison table
   - Download results as CSV
```

### Flow 3: Engineer - Production Training

```
1. Create New Experiment
   - Load config from YAML
   - Override specific parameters
   â†“
2. Validate Configuration
   - Check environment setup
   - Verify resource availability
   â†“
3. Start Training
   - Checkpoint every 10k steps
   - Auto-save on errors
   â†“
4. Monitor Training
   - Check loss curves for divergence
   - View system resource usage
   â†“
5. Training Complete
   - Export model checkpoint
   - Generate training report
```

## Design Patterns

### Pattern 1: Status Indicators

```
Running:    [â—] Training (45,000 / 100,000)  [Pause]
Paused:     [â¸] Training Paused              [Resume]
Completed:  [âœ“] Training Complete            [View Results]
Failed:     [âœ—] Training Failed              [View Logs]
Queued:     [â‹¯] Waiting for Resources        [Cancel]
```

### Pattern 2: Collapsible Sections

```
â–¸ Advanced Hyperparameters
  (Click to expand)

â–¾ Advanced Hyperparameters
  â”œâ”€ Entropy Coefficient: 0.01
  â”œâ”€ Value Function Coefficient: 0.5
  â”œâ”€ Max Gradient Norm: 0.5
  â””â”€ GAE Lambda: 0.95
```

### Pattern 3: Inline Validation

```
Learning Rate: [0.1_____]  âš ï¸ Warning: Value unusually high
                              Recommended: 0.0001 - 0.001

Batch Size:    [7_______]  âŒ Error: Must be power of 2
```

### Pattern 4: Smart Defaults

```
Environment: [CartPole-v1 â–¼]

Algorithm:   [PPO â–¼]           â„¹ï¸ Recommended for CartPole

             Other options:
             - DQN (also suitable)
             - SAC (for continuous actions)
```

## Accessibility Considerations

- **Color Blindness**: Use patterns/textures in addition to colors
- **Screen Readers**: Proper ARIA labels on all interactive elements
- **Keyboard Navigation**: All actions accessible via keyboard
- **High Contrast Mode**: Ensure visibility with system theme
- **Text Size**: Respect user's font size preferences

## Responsive Design

### Desktop (>1200px)
- Multi-column layout
- Detailed visualizations
- All information visible

### Tablet (768px - 1200px)
- Two-column layout
- Slightly simplified charts
- Collapsible sidebars

### Mobile (<768px)
- Single column
- Tabbed interface for sections
- Simplified visualizations
- Essential metrics only

## Design Checklist

Before finalizing UI design:

```
â˜ Clear visual hierarchy (primary info prominent)
â˜ Consistent spacing and alignment
â˜ Meaningful color usage (not decorative)
â˜ Loading states for async operations
â˜ Error states with actionable messages
â˜ Empty states with clear next actions
â˜ Tooltips for complex metrics
â˜ Keyboard shortcuts for common actions
â˜ Mobile-responsive layout
â˜ Accessibility compliance (WCAG 2.1 AA)
```

## Wireframe Template

```markdown
# UI Component: [Name]

## Purpose
[What user task does this support?]

## Layout
[ASCII wireframe]

## Key Elements
1. Element 1 - Purpose
2. Element 2 - Purpose

## Interactions
- User action â†’ System response

## States
- Default state
- Loading state
- Error state
- Success state

## Responsive Behavior
- Desktop: [Description]
- Tablet: [Description]
- Mobile: [Description]
```

---

**Ready to design!** Create intuitive, informative interfaces that help users train and understand RL agents effectively.
